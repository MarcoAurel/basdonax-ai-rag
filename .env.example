# Configuración del modelo LLM
# Opciones: phi3, llama3, mistral, etc.
# Para servidor sin GPU, se recomienda phi3
MODEL=phi3

# Modelo de embeddings para ChromaDB
# all-MiniLM-L6-v2 es ligero y efectivo
EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2

# Número de chunks de contexto a recuperar
TARGET_SOURCE_CHUNKS=5

# Host de Ollama (usar nombre del servicio en docker-compose)
OLLAMA_HOST=http://ollama:11434

# Configuración de ChromaDB
CHROMA_HOST=chroma
CHROMA_PORT=8000

# Puerto de la aplicación Streamlit
STREAMLIT_SERVER_PORT=8080
STREAMLIT_SERVER_ADDRESS=0.0.0.0
