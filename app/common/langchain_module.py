from langchain.chains import RetrievalQA
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_community.llms import Ollama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from common.chroma_db_settings import Chroma
from common.assistant_prompt import assistant_prompt
import os
import argparse

# Configuraci칩n del modelo
use_cloud_api = os.environ.get("USE_CLOUD_API", "false").lower() == "true"
cloud_provider = os.environ.get("CLOUD_PROVIDER", "groq")
model = os.environ.get("MODEL") if not use_cloud_api else os.environ.get("MODEL_NAME", "llama-3.1-70b-versatile")

# LOG: Mostrar configuraci칩n del modelo
print(f"游뱄 MODELO CONFIGURADO:")
print(f"   USE_CLOUD_API: {use_cloud_api}")
print(f"   CLOUD_PROVIDER: {cloud_provider}")
print(f"   MODEL: {model}")
print(f"   ---")

# For embeddings model, the example uses a sentence-transformers model
# https://www.sbert.net/docs/pretrained_models.html
# "The all-mpnet-base-v2 model provides the best quality, while all-MiniLM-L6-v2 is 5 times faster and still offers good quality."
embeddings_model_name = os.environ.get("EMBEDDINGS_MODEL_NAME", "all-MiniLM-L6-v2")
# OPTIMIZACI칍N: Menos chunks = respuestas m치s concisas
target_source_chunks = int(os.environ.get('TARGET_SOURCE_CHUNKS', 4))

from common.constants import get_chroma_client


def parse_arguments():
    """
    Parse argumentos de l칤nea de comandos.
    En Streamlit, usa valores por defecto si no hay args disponibles.
    """
    parser = argparse.ArgumentParser(description='privateGPT: Ask questions to your documents without an internet connection, '
                                                 'using the power of LLMs.')
    parser.add_argument("--hide-source", "-S", action='store_true',
                        help='Use this flag to disable printing of source documents used for answers.')

    parser.add_argument("--mute-stream", "-M",
                        action='store_true',
                        help='Use this flag to disable the streaming StdOut callback for LLMs.')

    try:
        return parser.parse_args()
    except SystemExit:
        # En Streamlit, parse_args() puede fallar, usar valores por defecto
        class DefaultArgs:
            hide_source = False
            mute_stream = True  # Mute por defecto en Streamlit
        return DefaultArgs()


def get_llm(callbacks):
    """Obtiene el LLM seg칰n la configuraci칩n (local u API en la nube)"""

    if use_cloud_api:
        # Usar API en la nube
        if cloud_provider == "groq":
            from langchain_groq import ChatGroq
            api_key = os.environ.get("GROQ_API_KEY")
            if not api_key:
                raise ValueError("GROQ_API_KEY no est치 configurada. Por favor agrega tu API key en el archivo .env")
            return ChatGroq(
                model=model,
                groq_api_key=api_key,
                temperature=0,
                max_tokens=300,  # Limitar respuestas a ~200 palabras
                callbacks=callbacks
            )

        elif cloud_provider == "openai":
            from langchain_openai import ChatOpenAI
            api_key = os.environ.get("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY no est치 configurada. Por favor agrega tu API key en el archivo .env")
            return ChatOpenAI(
                model=model,
                openai_api_key=api_key,
                temperature=0,
                callbacks=callbacks
            )

        elif cloud_provider == "anthropic":
            from langchain_anthropic import ChatAnthropic
            api_key = os.environ.get("ANTHROPIC_API_KEY")
            if not api_key:
                raise ValueError("ANTHROPIC_API_KEY no est치 configurada. Por favor agrega tu API key en el archivo .env")
            return ChatAnthropic(
                model=model,
                anthropic_api_key=api_key,
                temperature=0,
                callbacks=callbacks
            )

        elif cloud_provider == "google":
            from langchain_google_genai import ChatGoogleGenerativeAI
            api_key = os.environ.get("GOOGLE_API_KEY")
            if not api_key:
                raise ValueError("GOOGLE_API_KEY no est치 configurada. Por favor agrega tu API key en el archivo .env")
            return ChatGoogleGenerativeAI(
                model=model,
                google_api_key=api_key,
                temperature=0,
                callbacks=callbacks,
                transport="rest"  # Usar REST en lugar de gRPC para evitar problemas con event loop
            )

        else:
            raise ValueError(f"Proveedor no soportado: {cloud_provider}")
    else:
        # Usar Ollama local
        return Ollama(model=model, callbacks=callbacks, temperature=0, base_url='http://ollama:11434')


def response(query:str) -> str:
    # Parse the command line arguments
    args = parse_arguments()

    # activate/deactivate the streaming StdOut callback for LLMs
    callbacks = [] if args.mute_stream else [StreamingStdOutCallbackHandler()]

    # Obtener el LLM seg칰n configuraci칩n
    llm = get_llm(callbacks)

    # Intentar conectar a ChromaDB
    chroma_client = get_chroma_client()

    if chroma_client is None:
        # Si ChromaDB no est치 disponible, responder sin contexto RAG
        from langchain_core.prompts import ChatPromptTemplate

        simple_prompt = ChatPromptTemplate.from_messages([
            ("system", """Soy Au-Rex, el asistente virtual del departamento de TI de Luckia Arica, Chile.

IMPORTANTE: Actualmente no tengo acceso a la base de documentos RAG.
Solo puedo proporcionar informaci칩n general basada en mi conocimiento.

Para consultas espec칤ficas sobre documentaci칩n interna, procedimientos o manuales del departamento,
necesitar치s que el administrador configure la conexi칩n a ChromaDB."""),
            ("human", "{question}")
        ])

        chain = simple_prompt | llm | StrOutputParser()
        return chain.invoke({"question": query})

    # Si ChromaDB est치 disponible, usar RAG completo
    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)
    db = Chroma(client=chroma_client, embedding_function=embeddings)
    # Usar MMR para obtener chunks relevantes pero diversos
    # fetch_k busca entre m치s documentos, k devuelve los m치s diversos
    retriever = db.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": target_source_chunks,
            "fetch_k": target_source_chunks * 4,  # Buscar entre 4x m치s documentos
            "lambda_mult": 0.5  # Balance 50/50 relevancia y diversidad
        }
    )

    prompt = assistant_prompt()

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain.invoke(query)