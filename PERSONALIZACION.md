# Gu√≠a de Personalizaci√≥n

C√≥mo personalizar el asistente, cambiar modelos y optimizar para CPU.

## üé® Modificar el System Prompt

### Archivo a Editar
```
app/common/assistant_prompt.py
```

### Proceso
1. Abre el archivo en tu editor
2. Modifica el texto dentro de la funci√≥n `assistant_prompt()`
3. **Guarda el archivo**
4. **Recarga la p√°gina en el navegador** (F5 o Ctrl+R)

### ‚úÖ NO Necesitas
- ‚ùå Rebuild de Docker
- ‚ùå Reiniciar contenedores
- ‚ùå Redesplegar

### Ejemplo de Personalizaci√≥n

**Prompt Original:**
```python
def assistant_prompt():
    prompt = ChatPromptTemplate.from_messages(
    ("human", """ # Rol
     Sos la secretaria de PBC, tu nombre es Bastet...
```

**Prompt Personalizado:**
```python
def assistant_prompt():
    prompt = ChatPromptTemplate.from_messages(
    ("human", """ # Rol
     Eres un asistente especializado en analizar documentos t√©cnicos.
     Tu nombre es DocuBot.

    # Tarea
    Analiza el documento y responde de manera clara y precisa.

    Question: {question}
    Context: {context}

    # Instrucciones
    - S√© conciso y directo
    - Usa ejemplos cuando sea necesario
    - Responde solo bas√°ndote en el contexto proporcionado
    - Si no sabes algo, adm√≠telo
    """))
    return prompt
```

### Variables Importantes
- `{question}` - La pregunta del usuario
- `{context}` - El contexto recuperado de los documentos

**‚ö†Ô∏è NO elimines estas variables, son necesarias para el RAG**

---

## üöÄ Cambiar el Modelo LLM

### Modelos Recomendados para CPU (Sin GPU)

| Modelo | Tama√±o | Velocidad CPU | Calidad | Recomendado |
|--------|--------|---------------|---------|-------------|
| **tinyllama** | ~637MB | ‚ö°‚ö°‚ö°‚ö°‚ö° Muy r√°pido | ‚≠ê‚≠ê‚≠ê Buena | ‚úÖ **Mejor para CPU** |
| **phi3:mini** | ~2.3GB | ‚ö°‚ö°‚ö°‚ö° R√°pido | ‚≠ê‚≠ê‚≠ê‚≠ê Muy buena | ‚úÖ **Equilibrado** |
| **phi3** | ~2.5GB | ‚ö°‚ö°‚ö° Moderado | ‚≠ê‚≠ê‚≠ê‚≠ê Muy buena | Actual |
| **llama3.2:1b** | ~1.3GB | ‚ö°‚ö°‚ö°‚ö°‚ö° Muy r√°pido | ‚≠ê‚≠ê‚≠ê‚≠ê Muy buena | ‚úÖ **R√°pido y bueno** |
| **gemma2:2b** | ~1.6GB | ‚ö°‚ö°‚ö°‚ö° R√°pido | ‚≠ê‚≠ê‚≠ê‚≠ê Muy buena | ‚úÖ **Alternativa** |

### Proceso para Cambiar de Modelo

#### Paso 1: Descargar el Nuevo Modelo

```bash
# TinyLlama (el m√°s r√°pido)
docker exec basdonax-ollama-local ollama pull tinyllama

# Llama 3.2 1B (r√°pido y de buena calidad)
docker exec basdonax-ollama-local ollama pull llama3.2:1b

# Gemma2 2B
docker exec basdonax-ollama-local ollama pull gemma2:2b

# Phi3 mini
docker exec basdonax-ollama-local ollama pull phi3:mini
```

#### Paso 2: Verificar Modelos Descargados

```bash
docker exec basdonax-ollama-local ollama list
```

Deber√≠as ver algo como:
```
NAME              ID              SIZE      MODIFIED
phi3              latest          2.5 GB    2 hours ago
tinyllama         latest          637 MB    5 minutes ago
llama3.2:1b       latest          1.3 GB    3 minutes ago
```

#### Paso 3: Cambiar el Modelo en la Configuraci√≥n

Edita el archivo `docker-compose.local.yml`:

```yaml
environment:
  - MODEL=tinyllama  # Cambia aqu√≠
  - EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2
  - TARGET_SOURCE_CHUNKS=5
  - OLLAMA_HOST=http://ollama:11434
  - CHROMA_HOST=chroma
  - CHROMA_PORT=8000
```

**Opciones:**
- `MODEL=tinyllama` - El m√°s r√°pido
- `MODEL=llama3.2:1b` - R√°pido y buena calidad
- `MODEL=gemma2:2b` - Alternativa r√°pida
- `MODEL=phi3:mini` - Phi3 optimizado

#### Paso 4: Reiniciar el Contenedor UI

```bash
docker restart basdonax-ui-local
```

#### Paso 5: Verificar

Espera 10-20 segundos y recarga la p√°gina: `http://localhost:8080`

Prueba haciendo una pregunta para verificar la velocidad.

---

## ‚ö° Optimizar Velocidad (Sin Cambiar Modelo)

### Opci√≥n 1: Reducir el Contexto

Edita `docker-compose.local.yml`:

```yaml
environment:
  - MODEL=phi3
  - EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2
  - TARGET_SOURCE_CHUNKS=3  # Reducir de 5 a 3 (m√°s r√°pido)
```

Esto reduce la cantidad de informaci√≥n que el modelo procesa.

### Opci√≥n 2: Ajustar Temperatura

Edita `app/common/langchain_module.py` l√≠nea 46:

```python
llm = Ollama(model=model, callbacks=callbacks, temperature=0, base_url='http://ollama:11434')
```

Cambiar `temperature=0` a valores m√°s bajos puede hacer respuestas m√°s r√°pidas pero menos creativas.

**Despu√©s de este cambio, necesitas:**
```bash
docker restart basdonax-ui-local
```

---

## üîÑ Comparaci√≥n de Velocidad (CPU)

Tiempos aproximados de respuesta en CPU promedio (4 cores):

| Modelo | Tiempo Promedio | Tokens/seg |
|--------|-----------------|------------|
| tinyllama | ~2-4 seg | 30-50 |
| llama3.2:1b | ~3-5 seg | 20-40 |
| gemma2:2b | ~4-6 seg | 15-30 |
| phi3:mini | ~5-8 seg | 10-20 |
| phi3 (actual) | ~8-15 seg | 5-15 |
| llama3 (7B) | ~20-40 seg | 2-5 |

**Recomendaci√≥n:** Si Phi3 es muy lento, cambia a `llama3.2:1b` o `tinyllama`.

---

## üìä Otros Par√°metros Configurables

### Modelo de Embeddings

En `docker-compose.local.yml`:

```yaml
- EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2  # R√°pido, ligero
```

**Alternativas:**
- `all-MiniLM-L6-v2` - R√°pido, ligero (actual) ‚úÖ
- `all-mpnet-base-v2` - Mejor calidad, m√°s lento
- `paraphrase-multilingual-MiniLM-L12-v2` - Multiidioma

### Chunks de Contexto

```yaml
- TARGET_SOURCE_CHUNKS=5  # Cantidad de fragmentos a recuperar
```

- **M√°s chunks (7-10):** M√°s contexto, respuestas m√°s completas, M√ÅS LENTO
- **Menos chunks (2-3):** Menos contexto, respuestas m√°s r√°pidas, R√ÅPIDO

---

## üß™ Probando Diferentes Configuraciones

### Test 1: Modelo M√°s R√°pido

```yaml
environment:
  - MODEL=tinyllama
  - TARGET_SOURCE_CHUNKS=3
```

```bash
docker restart basdonax-ui-local
```

### Test 2: Balance Velocidad/Calidad

```yaml
environment:
  - MODEL=llama3.2:1b
  - TARGET_SOURCE_CHUNKS=4
```

```bash
docker restart basdonax-ui-local
```

### Test 3: M√°xima Calidad (M√°s Lento)

```yaml
environment:
  - MODEL=phi3
  - TARGET_SOURCE_CHUNKS=7
```

```bash
docker restart basdonax-ui-local
```

---

## üéØ Mi Recomendaci√≥n para CPU

### Para Desarrollo/Pruebas R√°pidas:
```yaml
environment:
  - MODEL=tinyllama
  - EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2
  - TARGET_SOURCE_CHUNKS=3
```

### Para Uso Normal:
```yaml
environment:
  - MODEL=llama3.2:1b
  - EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2
  - TARGET_SOURCE_CHUNKS=4
```

### Para Mejor Calidad (M√°s Paciencia):
```yaml
environment:
  - MODEL=phi3:mini
  - EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2
  - TARGET_SOURCE_CHUNKS=5
```

---

## üìù Resumen de Cambios

### ‚úÖ Cambios que NO Requieren Rebuild

- Modificar `assistant_prompt.py` ‚Üí Solo recarga la p√°gina
- Cambiar documentos ‚Üí Solo recarga

### ‚ö†Ô∏è Cambios que Requieren Reiniciar UI

- Cambiar `MODEL` en docker-compose
- Cambiar `TARGET_SOURCE_CHUNKS`
- Cambiar `EMBEDDINGS_MODEL_NAME`

**Comando:**
```bash
docker restart basdonax-ui-local
```

### üî® Cambios que Requieren Rebuild

- Modificar `requirements.txt`
- Modificar `Dockerfile`
- Agregar nuevas dependencias Python

**Comando:**
```bash
.\start-local.ps1 -Build
```

---

## üöÄ Prueba Ahora

**Mi recomendaci√≥n inmediata para ti:**

```bash
# 1. Descarga Llama 3.2 1B (r√°pido y buena calidad)
docker exec basdonax-ollama-local ollama pull llama3.2:1b

# 2. Edita docker-compose.local.yml y cambia:
#    - MODEL=llama3.2:1b

# 3. Reinicia
docker restart basdonax-ui-local

# 4. Espera 10-20 segundos y prueba
```

Deber√≠as notar una **mejora significativa en la velocidad** (3-5x m√°s r√°pido que Phi3).

---

## üìö Recursos Adicionales

- [Lista completa de modelos Ollama](https://ollama.com/library)
- [Ollama Model Cards](https://ollama.com/library) - Informaci√≥n detallada de cada modelo
- Experimenta con diferentes modelos seg√∫n tus necesidades

---

**¬øDudas?** Los cambios m√°s comunes son:
1. Prompt: Edita `assistant_prompt.py` ‚Üí Recarga p√°gina
2. Modelo: Edita `docker-compose.local.yml` ‚Üí Reinicia UI
